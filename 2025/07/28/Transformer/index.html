<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Transformer编码器 Encoder 输入：源序列 位置编码 记录位置信息  子层1: 自注意力机制 将输入序列的词变换为查询Q、键K、值V 权重矩阵 $W^Q, W^K, W^V$    多头注意力 将Q、K、V分割为多个头，分别独立计算不同的注意力，如语法、语义、指代等 并行计算 将结果通过一个线性层$W^O$拼接  注意力得分 $Attention(Q,K,V)&#x3D;softm">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="http://ishmaelheathcliff.github.io/2025/07/28/Transformer/index.html">
<meta property="og:site_name" content="絶望した">
<meta property="og:description" content="Transformer编码器 Encoder 输入：源序列 位置编码 记录位置信息  子层1: 自注意力机制 将输入序列的词变换为查询Q、键K、值V 权重矩阵 $W^Q, W^K, W^V$    多头注意力 将Q、K、V分割为多个头，分别独立计算不同的注意力，如语法、语义、指代等 并行计算 将结果通过一个线性层$W^O$拼接  注意力得分 $Attention(Q,K,V)&#x3D;softm">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lawrence-picgo.oss-cn-beijing.aliyuncs.com/images/Transformer%20architecture.png">
<meta property="article:published_time" content="2025-07-28T14:48:23.134Z">
<meta property="article:modified_time" content="2025-07-28T14:47:44.327Z">
<meta property="article:author" content="Ishamel Heathcliff">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lawrence-picgo.oss-cn-beijing.aliyuncs.com/images/Transformer%20architecture.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Transformer</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 7.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="顶部" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/IshmaelHeathcliff">项目</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="上一篇" href="/2025/07/28/Git%20%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="下一篇" href="/2025/07/28/%E6%9C%80%E5%B0%8F%E4%BD%9C%E7%94%A8%E9%87%8F%E5%8E%9F%E7%90%86/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&text=Transformer"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&is_video=false&description=Transformer"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Transformer&body=Check out this article: http://ishmaelheathcliff.github.io/2025/07/28/Transformer/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&name=Transformer&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&t=Transformer"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer"><span class="toc-number">1.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-Encoder"><span class="toc-number">1.1.</span> <span class="toc-text">编码器 Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%821-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.1.1.</span> <span class="toc-text">子层1: 自注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">多头注意力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%BE%97%E5%88%86"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">注意力得分</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%822-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.2.</span> <span class="toc-text">子层2: 前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">1.1.3.</span> <span class="toc-text">残差连接与层归一化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="toc-number">1.2.</span> <span class="toc-text">解码器 Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%821-%E6%8E%A9%E7%A0%81%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.2.1.</span> <span class="toc-text">子层1: 掩码自注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%822-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%B3%A8%E6%84%8F%E5%8A%9B%E3%80%81%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.2.2.</span> <span class="toc-text">子层2: 编码器-解码器注意力、交叉注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%823-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.2.3.</span> <span class="toc-text">子层3: 前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96-1"><span class="toc-number">1.2.4.</span> <span class="toc-text">残差连接与层归一化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82"><span class="toc-number">1.3.</span> <span class="toc-text">输出层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%EF%BC%9A%E4%BD%BF%E7%94%A8-Teacher-Forcing%EF%BC%8C%E7%9B%B4%E6%8E%A5%E8%AE%A1%E7%AE%97%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.3.1.</span> <span class="toc-text">训练阶段：使用 Teacher Forcing，直接计算交叉熵损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%EF%BC%9A"><span class="toc-number">1.3.2.</span> <span class="toc-text">推理阶段：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">1.4.</span> <span class="toc-text">结构</span></a></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        Transformer
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Ishamel Heathcliff</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2025-07-28T14:48:23.134Z" class="dt-published" itemprop="datePublished">2025-07-28</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/AI/" rel="tag">AI</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器 Encoder"></a>编码器 Encoder</h2><ul>
<li>输入：源序列</li>
<li><strong>位置编码</strong></li>
<li>记录位置信息</li>
</ul>
<h3 id="子层1-自注意力机制"><a href="#子层1-自注意力机制" class="headerlink" title="子层1: 自注意力机制"></a>子层1: <strong>自注意力机制</strong></h3><ul>
<li>将输入序列的词变换为查询Q、键K、值V<ul>
<li>权重矩阵 $W^Q, W^K, W^V$</li>
</ul>
</li>
</ul>
<h4 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h4><ul>
<li>将Q、K、V分割为多个头，分别独立计算不同的注意力，如语法、语义、指代等</li>
<li>并行计算</li>
<li>将结果通过一个线性层$W^O$拼接</li>
</ul>
<h4 id="注意力得分"><a href="#注意力得分" class="headerlink" title="注意力得分"></a>注意力得分</h4><ul>
<li>$Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V$<ul>
<li>$QK^T$ ​<ul>
<li>将所有查询 $Q$ 与所有键 $K$ ​进行<strong>点积</strong>计算相似度分数</li>
</ul>
</li>
<li>$\sqrt{d_k}$ ​<ul>
<li>缩放维持方差稳定，防止 $softmax$ ​后梯度消失</li>
</ul>
</li>
<li>$softmax$ ​<ul>
<li>将分数转化为概率分布</li>
</ul>
</li>
</ul>
</li>
<li>对每行进行</li>
<li>突出高相关性，抑制低相关性</li>
</ul>
<h3 id="子层2-前馈神经网络"><a href="#子层2-前馈神经网络" class="headerlink" title="子层2: 前馈神经网络"></a>子层2: 前馈神经网络</h3><ul>
<li>两层全连接，第一层使用ReLU激活函数，第二层不使用激活函数</li>
<li>$$<br>FFN(x)&#x3D;ReLU(xW_1+b_1)W_2+b_2<br>$$</li>
</ul>
<h3 id="残差连接与层归一化"><a href="#残差连接与层归一化" class="headerlink" title="残差连接与层归一化"></a>残差连接与层归一化</h3><ul>
<li>$$<br>x_{out}&#x3D;LayerNorm(x+Sublayer(x))<br>$$</li>
<li>每个子层后都要进行</li>
<li>残差连接 $x+Sublayer(x)$<ul>
<li>保留原始信息</li>
<li>缓解梯度消失</li>
</ul>
</li>
<li>归一化 $LayerNorm$<ul>
<li>加速训练，提升稳定性</li>
</ul>
</li>
<li>输出：源序列的上下文表示</li>
</ul>
<h2 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器 Decoder"></a>解码器 Decoder</h2><ul>
<li>输入：目标序列右移<ul>
<li><strong>Teacher Forching</strong>：训练时直接使用真实结果，可并行训练，加速收敛</li>
<li>推理时，使用之前的推理结果序列作为推理下一个单词的Decoder的输入</li>
</ul>
</li>
</ul>
<h3 id="子层1-掩码自注意力"><a href="#子层1-掩码自注意力" class="headerlink" title="子层1: 掩码自注意力"></a>子层1: <strong>掩码自注意力</strong></h3><ul>
<li>与编码器的自注意力机制类似，引入掩码</li>
<li>在$QK^T$中的$q_ik^T_j$，对$j&gt;i$的部分设为$-\infty$，softmax后为0，变为一个左下半矩阵</li>
<li>只关注序列中前面的词对该词的影响，防止未来信息泄露</li>
<li>输出：目标序列的上下文表示</li>
</ul>
<h3 id="子层2-编码器-解码器注意力、交叉注意力"><a href="#子层2-编码器-解码器注意力、交叉注意力" class="headerlink" title="子层2: 编码器-解码器注意力、交叉注意力"></a>子层2: <strong>编码器-解码器注意力</strong>、<strong>交叉注意力</strong></h3><ul>
<li>查询Q：来自解码器子层1掩码自注意力的输出，即目标序列的上下文表示</li>
<li>键K和值V：来自编码器的输出，即源序列的上下文表示</li>
<li>将目标序列与源序列对齐，既能解决目标序列内部依赖，也能获得源序列全局信息</li>
</ul>
<h3 id="子层3-前馈神经网络"><a href="#子层3-前馈神经网络" class="headerlink" title="子层3: 前馈神经网络"></a>子层3: 前馈神经网络</h3><h3 id="残差连接与层归一化-1"><a href="#残差连接与层归一化-1" class="headerlink" title="残差连接与层归一化"></a>残差连接与层归一化</h3><h2 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h2><ul>
<li>将解码器隐藏状态结果映射到词汇表大小维度</li>
<li>Softmax 概率化</li>
</ul>
<h3 id="训练阶段：使用-Teacher-Forcing，直接计算交叉熵损失"><a href="#训练阶段：使用-Teacher-Forcing，直接计算交叉熵损失" class="headerlink" title="训练阶段：使用 Teacher Forcing，直接计算交叉熵损失"></a><strong>训练阶段</strong>：使用 Teacher Forcing，直接计算交叉熵损失</h3><h3 id="推理阶段："><a href="#推理阶段：" class="headerlink" title="推理阶段："></a><strong>推理阶段</strong>：</h3><ul>
<li><strong>贪婪搜索（Greedy Search）</strong> ：选择每个位置概率最高的词</li>
<li><strong>束搜索（Beam Search）</strong> ：保留多个候选序列，平衡生成质量与多样性</li>
<li><strong>采样（Sampling）</strong> ：按概率分布随机采样（可加入温度调节）</li>
</ul>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><ul>
<li>多个编码器串联，再使用多个解码器串联</li>
<li>​<img src="https://lawrence-picgo.oss-cn-beijing.aliyuncs.com/images/Transformer%20architecture.png">​</li>
</ul>
<p>‍</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">首页</a></li>
        
          <li><a href="/about/">关于</a></li>
        
          <li><a href="/archives/">归档</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/IshmaelHeathcliff">项目</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer"><span class="toc-number">1.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-Encoder"><span class="toc-number">1.1.</span> <span class="toc-text">编码器 Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%821-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.1.1.</span> <span class="toc-text">子层1: 自注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">多头注意力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%BE%97%E5%88%86"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">注意力得分</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%822-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.2.</span> <span class="toc-text">子层2: 前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">1.1.3.</span> <span class="toc-text">残差连接与层归一化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="toc-number">1.2.</span> <span class="toc-text">解码器 Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%821-%E6%8E%A9%E7%A0%81%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.2.1.</span> <span class="toc-text">子层1: 掩码自注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%822-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%B3%A8%E6%84%8F%E5%8A%9B%E3%80%81%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.2.2.</span> <span class="toc-text">子层2: 编码器-解码器注意力、交叉注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E5%B1%823-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.2.3.</span> <span class="toc-text">子层3: 前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96-1"><span class="toc-number">1.2.4.</span> <span class="toc-text">残差连接与层归一化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82"><span class="toc-number">1.3.</span> <span class="toc-text">输出层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%EF%BC%9A%E4%BD%BF%E7%94%A8-Teacher-Forcing%EF%BC%8C%E7%9B%B4%E6%8E%A5%E8%AE%A1%E7%AE%97%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.3.1.</span> <span class="toc-text">训练阶段：使用 Teacher Forcing，直接计算交叉熵损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%EF%BC%9A"><span class="toc-number">1.3.2.</span> <span class="toc-text">推理阶段：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">1.4.</span> <span class="toc-text">结构</span></a></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&text=Transformer"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&is_video=false&description=Transformer"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Transformer&body=Check out this article: http://ishmaelheathcliff.github.io/2025/07/28/Transformer/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&title=Transformer"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&name=Transformer&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://ishmaelheathcliff.github.io/2025/07/28/Transformer/&t=Transformer"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2025
    Ishamel Heathcliff
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/IshmaelHeathcliff">项目</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板！\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功！");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
